{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation\n",
    "\n",
    "Recall the one-dimensional least squares setup. Here, we wish to relate $t_e$ to $d_e$ by $t_e \\approx \\beta_0 + \\beta_1d_e$, where $d$ is some function of $\\phi$. The goal is to minimize $$\\sum_{e \\in E_G}(t_e - \\beta_0 - \\beta_1d_e)^2$$ with respect to $\\beta_0$ and $\\beta_1$. Differentiating with respect to $\\beta_0$ and $\\beta_1$ gives the following relations: $$\\begin{aligned}\n",
    "    0 &= -2\\sum_{e \\in E_G}(t_e - \\beta_0 - \\beta_1d_e), \\\\\n",
    "    0 &= -2\\sum_{e \\in E_G}d_e(t_e - \\beta_0 - \\beta_1d_e).\n",
    "\\end{aligned}$$\n",
    "\n",
    "Rearranging, $$\\begin{aligned}\n",
    "    \\beta_1\\sum_{e \\in E_G}d_e &= \\sum_{e \\in E_G}t_e - \\beta_0|E_G|, \\\\\n",
    "    \\beta_1\\sum_{e \\in E_G}d_e^2 &= \\sum_{e \\in E_G}d_et_e - \\beta_0\\sum_{e \\in E_G}d_e.\n",
    "\\end{aligned}$$ Scaling and equating, we have $$\\begin{aligned}\n",
    "    \\left(\\sum_{e \\in E_G}d_e^2\\right)\\left(\\sum_{e \\in E_G}t_e\\right) - \\beta_0|E_G|\\left(\\sum_{e \\in E_G}d_e^2\\right) &= \\left(\\sum_{e \\in E_G}d_e\\right)\\left(\\sum_{e \\in E_G}d_et_e\\right) - \\beta_0\\left(\\sum_{e \\in E_G}d_e\\right)^2 \\\\\n",
    "    \\beta_0 &= \\frac{\\left(\\sum_{e \\in E_G}d_e^2\\right)\\left(\\sum_{e \\in E_G}t_e\\right) - \\left(\\sum_{e \\in E_G}d_e\\right)\\left(\\sum_{e \\in E_G}d_et_e\\right)}{|E_G|\\left(\\sum_{e \\in E_G}d_e^2\\right) - \\left(\\sum_{e \\in E_G}d_e\\right)^2}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "We similarly find $$\\begin{aligned}\n",
    "    \\beta_0|E_G| &= \\sum_{e \\in E_G}t_e - \\beta_1\\sum_{e \\in E_G}d_e, \\\\\n",
    "    \\beta_0\\sum_{e \\in E_G}d_e &= \\sum_{e \\in E_G}d_et_e - \\beta_1\\sum_{e \\in E_G}d_e^2,\n",
    "\\end{aligned}$$ so $$\\begin{aligned}\n",
    "    \\left(\\sum_{e \\in E_G}d_e\\right)\\left(\\sum_{e \\in E_G}t_e\\right) - \\beta_1\\left(\\sum_{e \\in E_G}d_e\\right)^2 &= |E_G|\\sum_{e \\in E_G}d_et_e - \\beta_1|E_G|\\sum_{e \\in E_G}d_e^2 \\\\\n",
    "    \\beta_1 &= \\frac{|E_G|\\sum_{e \\in E_G}d_et_e - \\left(\\sum_{e \\in E_G}d_e\\right)\\left(\\sum_{e \\in E_G}t_e\\right)}{|E_G|\\left(\\sum_{e \\in E_G}d_e^2\\right) - \\left(\\sum_{e \\in E_G}d_e\\right)^2}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "These expressions are quite bad, but we can simplify them somewhat if we make the following assumptions: $$\\begin{aligned}\n",
    "    1 &= \\frac{1}{|E_G|}\\sum_{e \\in E_G}d_e^2 , \\\\\n",
    "    0 &= \\sum_{e \\in E_G}d_e.\n",
    "\\end{aligned}$$ Then $$\\begin{aligned}\n",
    "    \\beta_0 &= \\frac{1}{|E_G|}\\sum_{e \\in E_G}t_e, \\\\\n",
    "    \\beta_1 &= \\frac{1}{|E_G|}\\sum_{e \\in E_G}d_et_e.\n",
    "\\end{aligned}$$ The assumptions hold if we have $$\\begin{aligned}\n",
    "    \\widetilde{d}_e &\\triangleq \\phi_e - \\frac{1}{|E_G|}\\sum_{e' \\in E_G}\\phi_{e'}, \\\\\n",
    "    d_e &\\triangleq \\frac{\\widetilde{d}_e}{\\sqrt{\\frac{1}{|E_G|}\\sum_{e' \\in E_G}\\widetilde{d}_{e'}^2}}.\n",
    "\\end{aligned}$$ Importantly, these are just affine transformations.\n",
    "\n",
    "Then $$\\mathcal{L}_{\\mathrm{geodesic}}(M) \\triangleq \\sum_{e \\in E_G}\\left(t_e - \\frac{1}{|E_G|}\\sum_{e' \\in E_G}t_{e'} - \\frac{d_e}{|E_G|}\\sum_{e' \\in E_G}d_{e'}t_{e'}\\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forward:\n",
    "    def __init__(self):\n",
    "        self._phi = None\n",
    "        self._t = None\n",
    "\n",
    "        self.d_tilde = None\n",
    "        self.d = None\n",
    "        self.residuals = None\n",
    "        self.lse = None\n",
    "\n",
    "        self.beta = None\n",
    "\n",
    "    def calc_lse(self, phi, t):\n",
    "        E = phi.shape[0]\n",
    "\n",
    "        # If phi and t haven't changed, don't do any additional work.\n",
    "        if (self._phi is not None and self._phi.shape == phi.shape\n",
    "            and np.allclose(self._phi, phi)\n",
    "            and self._t is not None and self._t.shape == t.shape\n",
    "            and np.allclose(self._t, t)\n",
    "            and self.lse is not None):\n",
    "            return self.lse\n",
    "\n",
    "        self._phi = np.copy(phi)\n",
    "        self._t = np.copy(t)\n",
    "\n",
    "        self.d_tilde = phi - np.sum(phi) / E\n",
    "        self.d = self.d_tilde / (self.d_tilde @ self.d_tilde / E)**0.5\n",
    "        self.residuals = t - np.sum(t) / E - (self.d @ t / E) * self.d\n",
    "        self.lse = self.residuals @ self.residuals\n",
    "        return self.lse\n",
    "\n",
    "    def calc_beta(self, phi, t):\n",
    "        E = phi.shape[0]\n",
    "\n",
    "        # If phi and t haven't changed, don't do any additional work.\n",
    "        if (self._phi is not None and self._phi.shape == phi.shape\n",
    "            and np.allclose(self._phi, phi)\n",
    "            and self._t is not None and self._t.shape == t.shape\n",
    "            and np.allclose(self._t, t)\n",
    "            and self.beta is not None):\n",
    "            return self.beta\n",
    "\n",
    "        # Note that the following disagrees with the notation in the writeup\n",
    "        # above. In particular, beta here is the coefficients when relating phi\n",
    "        # to t (as opposed to relating d to t).\n",
    "        denominator = E * (phi @ phi) - np.sum(phi)**2\n",
    "        self.beta = (\n",
    "            (phi @ phi * np.sum(t) - np.sum(phi) * (phi @ t)) / denominator,\n",
    "            (E * (phi @ t) - np.sum(phi) * np.sum(t)) / denominator,\n",
    "        )\n",
    "        return self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Computation\n",
    "\n",
    "From the above, we get, $$\\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial \\rho_\\ell}\\mathcal{L}_{\\mathrm{geodesic}}(M) &= -\\frac{2}{|E_G|}\\sum_{e \\in E_G}\\left(t_e - \\frac{1}{|E_G|}\\sum_{e' \\in E_G}t_{e'} - \\frac{d_e}{|E_G|}\\sum_{e' \\in E_G}d_{e'}t_{e'}\\right) \\\\\n",
    "        &\\hspace{8em}\\cdot \\left(\\frac{\\partial d_e}{\\partial \\rho_\\ell}\\sum_{e' \\in E_G}d_{e'}t_{e'} + d_e\\sum_{e' \\in E_G}\\frac{\\partial d_{e'}}{\\partial \\rho_\\ell}t_{e'}\\right), \\\\\n",
    "    \\frac{\\partial d_e}{\\partial \\rho_\\ell} &= \\frac{\\sqrt{\\sum_{e' \\in E_G}\\widetilde{d}_{e'}^2} \\cdot \\frac{\\partial \\widetilde{d}_e}{\\partial \\rho_\\ell} - d_e\\sum_{e' \\in E_G}\\left(\\widetilde{d}_{e'} \\cdot \\frac{\\partial \\widetilde{d}_{e'}}{\\partial \\rho_\\ell}\\right)}{\\sum_{e' \\in E_G}\\widetilde{d}_{e'}^2}, \\\\\n",
    "        &= \\frac{1}{\\sqrt{\\sum_{e' \\in E_G}\\widetilde{d}_{e'}^2}}\\left(\\frac{\\partial \\widetilde{d}_e}{\\partial \\rho_\\ell} - d_e\\sum_{e' \\in E_G}\\left(d_{e'} \\cdot \\frac{\\partial \\widetilde{d}_{e'}}{\\partial \\rho_\\ell}\\right)\\right) \\\\\n",
    "    \\frac{\\partial \\widetilde{d}_e}{\\partial \\rho_\\ell} &= \\frac{\\partial \\phi_e}{\\partial \\rho_\\ell} - \\frac{1}{|E_G|}\\sum_{e' \\in E_G}\\frac{\\partial \\phi_{e'}}{\\partial \\rho_\\ell}.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reverse:\n",
    "    def __init__(self, linear_regression_forward=None):\n",
    "        self._phi = None\n",
    "        self._t = None\n",
    "        self._dif_phi = None\n",
    "\n",
    "        self._ls = None\n",
    "\n",
    "        self._linear_regression_forward = linear_regression_forward\n",
    "        if linear_regression_forward is None:\n",
    "            self._linear_regression_forward = Forward()\n",
    "\n",
    "        self._d_tilde = None\n",
    "        self._d = None\n",
    "        self._residuals = None\n",
    "        self._lse = None\n",
    "\n",
    "        self.dif_d_tilde = None\n",
    "        self.dif_d = None\n",
    "        self.dif_residuals = None\n",
    "        self.dif_lse = None\n",
    "\n",
    "    def calc_dif_lse(self, phi, t, dif_phi, ls=None):\n",
    "        E = phi.shape[0]\n",
    "        if self._ls is None:\n",
    "            self._ls = range(E)\n",
    "        if ls is None:\n",
    "            ls = range(E)\n",
    "\n",
    "        pair_changed = False\n",
    "        if (self._phi is None or self._phi.shape != phi.shape\n",
    "            or not np.allclose(self._phi, phi)\n",
    "            or self._t is None or self._t.shape != t.shape\n",
    "            or not np.allclose(self._t, t) or list(self._ls) != list(ls)\n",
    "            or self._dif_phi is None\n",
    "            or not np.all([l in self._dif_phi for l in self._ls])\n",
    "            or not np.all([l in dif_phi for l in self._ls])\n",
    "            or not np.all([np.allclose(self._dif_phi[l], dif_phi[l])\n",
    "                           for l in self._ls])):\n",
    "            pair_changed = True\n",
    "            self._phi = np.copy(phi)\n",
    "            self._t = np.copy(t)\n",
    "            self._dif_phi = dif_phi\n",
    "\n",
    "            self._ls = ls\n",
    "\n",
    "            self._linear_regression_forward.calc_lse(self._phi, self._t)\n",
    "            self._d_tilde = self._linear_regression_forward.d_tilde\n",
    "            self._d = self._linear_regression_forward.d\n",
    "            self._residuals = self._linear_regression_forward.residuals\n",
    "            self._lse = self._linear_regression_forward.lse\n",
    "\n",
    "        if not pair_changed:\n",
    "            return self.dif_lse\n",
    "\n",
    "        self.dif_d_tilde = {l: self._dif_phi[l] - np.sum(self._dif_phi[l]) / E for l in self._ls}\n",
    "        self.dif_d = {l: (self.dif_d_tilde[l] - (self._d @ self.dif_d_tilde[l]) * self._d) / linalg.norm(self._d_tilde) for l in self._ls}\n",
    "        self.dif_residuals = {l: (-self._d @ self._t * self.dif_d[l] - self.dif_d[l] @ self._t * self._d) / E for l in self._ls}\n",
    "        self.dif_lse = {l: 2 * self._residuals @ self.dif_residuals[l] for l in self._ls}\n",
    "        return self.dif_lse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
